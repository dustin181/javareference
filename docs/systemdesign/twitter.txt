1) Functional Requirements
a) post a tweet
b) delete a tweet, but not edit or update
c) Max Size: 140 chars
d) users can like a tweet
e) user can follow another user (see posts from user b on home timeline)
f) two types of timelines: User timeline (all tweets user has posted) and Home Timeline (all tweets from other users that a user is following)
g) search tweets
h) users need an account to post tweets
i) text tweets only?
j) analytics/monitoring
k) recommendations
l) reply or share a tweet
m) tweet push notifications
n) trending topics

2) Nonfunctional requirements
a) Highly Available
b) Scalable
c) Durability of Data
d) Minimal Latency
e) Eventual Consistency
f) 

3) APIs
a) Login
b) Logoff
c) postTweet(userToken, tweet) --> create tweetid --> insert tweetid in tweet table --> forward to fan out service and user timeline service (synchronously)
d) deleteTweet(userToken, tweet_id)
e) likeOrUnlikeTweet(tweet_id, boolean like)
f) followUser(userToken, userIdToFollow) --> and readUserFollowers and readuserFollowee
g) readHomeTimeline(userToken, optional pageToken, optional pagesize)
h) readUserTimeLine(userToken, optional pageToken, optional pagesize)
i) searchTweets(userToken, searchTerms, maxCount, pageToken)


4) Services

Analytics
Search
Tweet
UserTimeline
HomeTimeline
Social Graph
Fan Out

5) Schema

Users:
PK userId: integer
name: varchar
email: varchar
creationTime: dateTime
lastLogin: dateTime
isHotUser: boolean

Tweets:
PK tweetId varchar
SK userId: integer
SK creationTime: dateTime
content varchar

Favorite_tweet:
PK tweet_id varchar
PK userid: integer
creationTime: dateTime

User_Relation:
PK followerUserId: integer
PK followeeUserId: integer
creationTime: dateTime

5) Datastore

approach 1) shard tweets by userid.  All tweets from user to single partition.  Scale up read replicas in that case.  W + R > N --> 4 + 2 > 6
approach 2) shard by tweetid.  Query all partitions for that particular user and they will return tweets from that user and then aggregate it on a central server/service.
approach 3) shard by userid and tweetid.  Query small number of partitions to read tweets for that user.

6) UserTimeLine Service:
a) in-memory hashmap with a double linked list that has the tweets in chronological order.  O(1) to add a new tweet at the head.

7) Social Graph Service:
a) keeps track of which users are following each other and it is used by the fan out service.  Graph database possibly?

8) Fanout service:
Query social graph service to find users who are following and push out the messages to the distributed (fifo?) queues to be consumed.  

9) Home Timeline Service:
a) in-memory hashmap with a double linked list that has the tweets in chronological order.  O(1) to add a new tweet at the head.

10) Search Service:
Fanout service --> Ingester removes stop words and reduce down to root words --> Process into datastore with cache.  Stored as primary key of word/tweetId.  Also with creationTime.
shard approach 1: word --> shard by hash of word.  Different words go to different partitions and then a single partition will hold all the tweets that contain that word.  To find tweet containing that word we will go to a single 
partition so search is fast, but the call from the upstream search service can be slower because it has to aggregate all these different words that make up a tweet which come from multiple partitions.  However, if you get a hot word, you will get a large number of queries to that partition, you will need a distributed cache in front of the datastore and it has to scale up and add additional read replicas for that particular partition..
shard approach 2: tweet id --> write operation to single partition.  Write will be very fast because it just goes to a single partition.  However, search will be slower because it has to go to all the partitions to read the list of tweets which contain the specific word and then an aggregator server has to combine the results before sending to user.  So, every single search hits all partitions, which puts extra pressure on the datastore.
shard approach 3: word/tweetid --> two level shard by words and tweetid.  First layer sharding by hash of word to a group of partitions and second layer sharding by tweet id to determine a partition in that group where the word to tweet id mapping should be store.  To search tweets for a word, we only need to go to a small set of partitions.

Blender on the other side is the same as the ingester --> removes stop words from user query and reduce down to root words then query the datastore




